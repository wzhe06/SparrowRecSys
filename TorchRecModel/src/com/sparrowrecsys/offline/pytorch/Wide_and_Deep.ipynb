{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide and Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and test sample, remove unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home/leon/Documents/SparrowRecSys/src/main/resources/webroot/sampledata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = folder_path + \"/Pytorch_data/trainingSamples.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = folder_path + \"/Pytorch_data/testSamples.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv(training_path, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_path, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>releaseYear</th>\n",
       "      <th>movieGenre1</th>\n",
       "      <th>movieGenre2</th>\n",
       "      <th>movieGenre3</th>\n",
       "      <th>movieRatingCount</th>\n",
       "      <th>...</th>\n",
       "      <th>userGenre3</th>\n",
       "      <th>userGenre4</th>\n",
       "      <th>userGenre5</th>\n",
       "      <th>scaledReleaseYear</th>\n",
       "      <th>scaledmovieRatingCount</th>\n",
       "      <th>scaledmovieAvgRating</th>\n",
       "      <th>scaledmovieRatingStddev</th>\n",
       "      <th>scaleduserRatingCount</th>\n",
       "      <th>scaleduserAvgRating</th>\n",
       "      <th>scaleduserRatingStddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>593</td>\n",
       "      <td>10096</td>\n",
       "      <td>4.0</td>\n",
       "      <td>954365552</td>\n",
       "      <td>1</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>13692.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.936777</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.449735</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.279874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>832</td>\n",
       "      <td>10351</td>\n",
       "      <td>3.0</td>\n",
       "      <td>851791379</td>\n",
       "      <td>0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3052.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0.985916</td>\n",
       "      <td>0.208758</td>\n",
       "      <td>0.649306</td>\n",
       "      <td>0.486773</td>\n",
       "      <td>0.112245</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.229560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85</td>\n",
       "      <td>10351</td>\n",
       "      <td>3.0</td>\n",
       "      <td>851791395</td>\n",
       "      <td>0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>592.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.040438</td>\n",
       "      <td>0.690972</td>\n",
       "      <td>0.502645</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.229560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>588</td>\n",
       "      <td>10351</td>\n",
       "      <td>5.0</td>\n",
       "      <td>851792205</td>\n",
       "      <td>1</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>8980.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.614369</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.486773</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.675555</td>\n",
       "      <td>0.207547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>370</td>\n",
       "      <td>1090</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1117852491</td>\n",
       "      <td>0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3087.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957747</td>\n",
       "      <td>0.211153</td>\n",
       "      <td>0.482639</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.204403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId  userId  rating   timestamp  label  releaseYear  movieGenre1  \\\n",
       "0      593   10096     4.0   954365552      1       1991.0           13   \n",
       "1      832   10351     3.0   851791379      0       1996.0           13   \n",
       "2       85   10351     3.0   851791395      0       1995.0           11   \n",
       "3      588   10351     5.0   851792205      1       1992.0            3   \n",
       "4      370    1090     2.0  1117852491      0       1994.0            2   \n",
       "\n",
       "   movieGenre2  movieGenre3  movieRatingCount  ...  userGenre3  userGenre4  \\\n",
       "0            4           12           13692.0  ...          17          12   \n",
       "1           12            0            3052.0  ...           5          12   \n",
       "2            5            0             592.0  ...           5          12   \n",
       "3           15           18            8980.0  ...          12           5   \n",
       "4            7            0            3087.0  ...           0           0   \n",
       "\n",
       "   userGenre5  scaledReleaseYear  scaledmovieRatingCount  \\\n",
       "0           0           0.915493                0.936777   \n",
       "1          13           0.985916                0.208758   \n",
       "2          13           0.971831                0.040438   \n",
       "3          13           0.929577                0.614369   \n",
       "4           0           0.957747                0.211153   \n",
       "\n",
       "   scaledmovieAvgRating  scaledmovieRatingStddev  scaleduserRatingCount  \\\n",
       "0              0.906250                 0.449735               0.030612   \n",
       "1              0.649306                 0.486773               0.112245   \n",
       "2              0.690972                 0.502645               0.122449   \n",
       "3              0.729167                 0.486773               0.224490   \n",
       "4              0.482639                 0.555555               0.030612   \n",
       "\n",
       "   scaleduserAvgRating  scaleduserRatingStddev  \n",
       "0             0.688889                0.279874  \n",
       "1             0.726667                0.229560  \n",
       "2             0.713333                0.229560  \n",
       "3             0.675555                0.207547  \n",
       "4             0.200000                0.204403  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN items in 'userRatedMovie1' column, movieId starts from 1, so we can use 0 to do padding\n",
    "test_df.fillna(0, inplace=True) \n",
    "training_df.fillna(0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns2Keep = ['userId', 'userGenre1', 'userGenre2',  'userGenre3','userGenre4', 'userGenre5', 'scaleduserRatingCount',\n",
    "       'scaleduserAvgRating', 'scaleduserRatingStddev', 'userRatedMovie1', 'movieId',  'movieGenre1', 'movieGenre2', 'movieGenre3', 'scaledReleaseYear', 'scaledmovieRatingCount', 'scaledmovieAvgRating',\n",
    "       'scaledmovieRatingStddev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_feature = training_df[columns2Keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_label = training_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature = test_df[columns2Keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_feature['userRatedMovie1'] = training_feature['userRatedMovie1'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature['userRatedMovie1'] = test_feature['userRatedMovie1'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> userId\n",
      "1 -> userGenre1\n",
      "2 -> userGenre2\n",
      "3 -> userGenre3\n",
      "4 -> userGenre4\n",
      "5 -> userGenre5\n",
      "6 -> scaleduserRatingCount\n",
      "7 -> scaleduserAvgRating\n",
      "8 -> scaleduserRatingStddev\n",
      "9 -> userRatedMovie1\n",
      "10 -> movieId\n",
      "11 -> movieGenre1\n",
      "12 -> movieGenre2\n",
      "13 -> movieGenre3\n",
      "14 -> scaledReleaseYear\n",
      "15 -> scaledmovieRatingCount\n",
      "16 -> scaledmovieAvgRating\n",
      "17 -> scaledmovieRatingStddev\n"
     ]
    }
   ],
   "source": [
    "for i, col_name in enumerate(training_feature.columns):\n",
    "    print(str(i) + \" -> \" + col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>userGenre1</th>\n",
       "      <th>userGenre2</th>\n",
       "      <th>userGenre3</th>\n",
       "      <th>userGenre4</th>\n",
       "      <th>userGenre5</th>\n",
       "      <th>scaleduserRatingCount</th>\n",
       "      <th>scaleduserAvgRating</th>\n",
       "      <th>scaleduserRatingStddev</th>\n",
       "      <th>userRatedMovie1</th>\n",
       "      <th>movieId</th>\n",
       "      <th>movieGenre1</th>\n",
       "      <th>movieGenre2</th>\n",
       "      <th>movieGenre3</th>\n",
       "      <th>scaledReleaseYear</th>\n",
       "      <th>scaledmovieRatingCount</th>\n",
       "      <th>scaledmovieAvgRating</th>\n",
       "      <th>scaledmovieRatingStddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10096</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.279874</td>\n",
       "      <td>50</td>\n",
       "      <td>593</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.936777</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.449735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10351</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0.112245</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.229560</td>\n",
       "      <td>26</td>\n",
       "      <td>832</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985916</td>\n",
       "      <td>0.208758</td>\n",
       "      <td>0.649306</td>\n",
       "      <td>0.486773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10351</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.229560</td>\n",
       "      <td>26</td>\n",
       "      <td>85</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.040438</td>\n",
       "      <td>0.690972</td>\n",
       "      <td>0.502645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10351</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.675555</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>582</td>\n",
       "      <td>588</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.614369</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.486773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1090</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.204403</td>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957747</td>\n",
       "      <td>0.211153</td>\n",
       "      <td>0.482639</td>\n",
       "      <td>0.555555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84480</th>\n",
       "      <td>9515</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.684444</td>\n",
       "      <td>0.270440</td>\n",
       "      <td>153</td>\n",
       "      <td>485</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.943662</td>\n",
       "      <td>0.196989</td>\n",
       "      <td>0.420139</td>\n",
       "      <td>0.529101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84481</th>\n",
       "      <td>9515</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.418367</td>\n",
       "      <td>0.668889</td>\n",
       "      <td>0.261006</td>\n",
       "      <td>153</td>\n",
       "      <td>720</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.985916</td>\n",
       "      <td>0.123298</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.550265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84482</th>\n",
       "      <td>9515</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.671111</td>\n",
       "      <td>0.257862</td>\n",
       "      <td>720</td>\n",
       "      <td>296</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0.957747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.518518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84483</th>\n",
       "      <td>9515</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.682222</td>\n",
       "      <td>0.261006</td>\n",
       "      <td>527</td>\n",
       "      <td>318</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957747</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84484</th>\n",
       "      <td>9515</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.459184</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.267296</td>\n",
       "      <td>318</td>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.699282</td>\n",
       "      <td>0.965278</td>\n",
       "      <td>0.396825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84485 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  userGenre1  userGenre2  userGenre3  userGenre4  userGenre5  \\\n",
       "0       10096          13          11          17          12           0   \n",
       "1       10351          11           7           5          12          13   \n",
       "2       10351          11           7           5          12          13   \n",
       "3       10351          11           7          12           5          13   \n",
       "4        1090           0           0           0           0           0   \n",
       "...       ...         ...         ...         ...         ...         ...   \n",
       "84480    9515           2          12          13           3           7   \n",
       "84481    9515           2          12          13           3           7   \n",
       "84482    9515           2          12          13           3           7   \n",
       "84483    9515           2          12          13           3           7   \n",
       "84484    9515           2          12          13           3          11   \n",
       "\n",
       "       scaleduserRatingCount  scaleduserAvgRating  scaleduserRatingStddev  \\\n",
       "0                   0.030612             0.688889                0.279874   \n",
       "1                   0.112245             0.726667                0.229560   \n",
       "2                   0.122449             0.713333                0.229560   \n",
       "3                   0.224490             0.675555                0.207547   \n",
       "4                   0.030612             0.200000                0.204403   \n",
       "...                      ...                  ...                     ...   \n",
       "84480               0.367347             0.684444                0.270440   \n",
       "84481               0.418367             0.668889                0.261006   \n",
       "84482               0.428571             0.671111                0.257862   \n",
       "84483               0.448980             0.682222                0.261006   \n",
       "84484               0.459184             0.688889                0.267296   \n",
       "\n",
       "       userRatedMovie1  movieId  movieGenre1  movieGenre2  movieGenre3  \\\n",
       "0                   50      593           13            4           12   \n",
       "1                   26      832           13           12            0   \n",
       "2                   26       85           11            5            0   \n",
       "3                  582      588            3           15           18   \n",
       "4                    0      370            2            7            0   \n",
       "...                ...      ...          ...          ...          ...   \n",
       "84480              153      485            2            3            7   \n",
       "84481              153      720            3           15            7   \n",
       "84482              720      296            7           13           11   \n",
       "84483              527      318           13           11            0   \n",
       "84484              318       50           13           17           12   \n",
       "\n",
       "       scaledReleaseYear  scaledmovieRatingCount  scaledmovieAvgRating  \\\n",
       "0               0.915493                0.936777              0.906250   \n",
       "1               0.985916                0.208758              0.649306   \n",
       "2               0.971831                0.040438              0.690972   \n",
       "3               0.929577                0.614369              0.729167   \n",
       "4               0.957747                0.211153              0.482639   \n",
       "...                  ...                     ...                   ...   \n",
       "84480           0.943662                0.196989              0.420139   \n",
       "84481           0.985916                0.123298              0.861111   \n",
       "84482           0.957747                1.000000              0.902778   \n",
       "84483           0.957747                0.945946              1.000000   \n",
       "84484           0.971831                0.699282              0.965278   \n",
       "\n",
       "       scaledmovieRatingStddev  \n",
       "0                     0.449735  \n",
       "1                     0.486773  \n",
       "2                     0.502645  \n",
       "3                     0.486773  \n",
       "4                     0.555555  \n",
       "...                        ...  \n",
       "84480                 0.529101  \n",
       "84481                 0.550265  \n",
       "84482                 0.518518  \n",
       "84483                 0.380952  \n",
       "84484                 0.396825  \n",
       "\n",
       "[84485 rows x 18 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossfeature label encoding for the wide part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_col = [0, 1, 2, 3, 4, 5, 10, 11, 12, 13] # column_index of sparse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_col_size = [30001, 20, 20, 20, 20, 20, 1001, 20, 20, 20] # number of classes per sparse_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_col = [6, 7, 8, 14, 15, 16, 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_col = [(9, 10)] # only 1 pair of crossed_feature here, you can add yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cross_feature(df, cross_col, encoding_start):\n",
    "    # given a dataframe df and cross column tuple, return the {(feaure pair): label_encoding} mapping\n",
    "    # e.g. return {(13, 34): 18}\n",
    "    feature_name_to_label = {}\n",
    "    label = encoding_start\n",
    "    for col_1_index, col_2_index in cross_col:\n",
    "        for cell_1, cell_2 in zip(df.iloc[:, col_1_index], df.iloc[:, col_2_index]):\n",
    "            feature_name = str(col_1_index) + \"_\" + str(col_2_index) + \"_\" + str(cell_1) + \"_\" + str(cell_2)\n",
    "            if feature_name not in feature_name_to_label:\n",
    "                feature_name_to_label[feature_name] = label\n",
    "                label += 1\n",
    "    return feature_name_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name_to_label = encode_cross_feature(training_feature, cross_col, 20)\n",
    "# we have used 0-19 labels, now start from 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'9_10_50_593': 20,\n",
       " '9_10_26_832': 21,\n",
       " '9_10_26_85': 22,\n",
       " '9_10_582_588': 23,\n",
       " '9_10_0_370': 24,\n",
       " '9_10_480_648': 25,\n",
       " '9_10_924_750': 26,\n",
       " '9_10_32_903': 27,\n",
       " '9_10_356_253': 28,\n",
       " '9_10_628_733': 29,\n",
       " '9_10_786_809': 30,\n",
       " '9_10_786_880': 31,\n",
       " '9_10_110_288': 32,\n",
       " '9_10_253_47': 33,\n",
       " '9_10_410_300': 34,\n",
       " '9_10_266_432': 35,\n",
       " '9_10_350_236': 36,\n",
       " '9_10_350_553': 37,\n",
       " '9_10_553_508': 38,\n",
       " '9_10_186_44': 39,\n",
       " '9_10_44_158': 40,\n",
       " '9_10_293_168': 41,\n",
       " '9_10_780_353': 42,\n",
       " '9_10_858_673': 43,\n",
       " '9_10_296_592': 44,\n",
       " '9_10_590_588': 45,\n",
       " '9_10_1_110': 46,\n",
       " '9_10_34_290': 47,\n",
       " '9_10_150_36': 48,\n",
       " '9_10_261_529': 49,\n",
       " '9_10_475_685': 50,\n",
       " '9_10_685_345': 51,\n",
       " '9_10_356_292': 52,\n",
       " '9_10_292_318': 53,\n",
       " '9_10_47_500': 54,\n",
       " '9_10_296_150': 55,\n",
       " '9_10_329_316': 56,\n",
       " '9_10_593_457': 57,\n",
       " '9_10_34_480': 58,\n",
       " '9_10_50_32': 59,\n",
       " '9_10_141_509': 60,\n",
       " '9_10_265_474': 61,\n",
       " '9_10_474_273': 62,\n",
       " '9_10_232_540': 63,\n",
       " '9_10_247_519': 64,\n",
       " '9_10_229_191': 65,\n",
       " '9_10_171_242': 66,\n",
       " '9_10_242_458': 67,\n",
       " '9_10_458_369': 68,\n",
       " '9_10_457_480': 69,\n",
       " '9_10_32_50': 70,\n",
       " '9_10_50_172': 71,\n",
       " '9_10_497_485': 72,\n",
       " '9_10_778_800': 73,\n",
       " '9_10_125_535': 74,\n",
       " '9_10_125_448': 75,\n",
       " '9_10_372_415': 76,\n",
       " '9_10_902_916': 77,\n",
       " '9_10_937_322': 78,\n",
       " '9_10_417_745': 79,\n",
       " '9_10_745_866': 80,\n",
       " '9_10_750_541': 81,\n",
       " '9_10_858_911': 82,\n",
       " '9_10_81_21': 83,\n",
       " '9_10_81_613': 84,\n",
       " '9_10_81_440': 85,\n",
       " '9_10_832_149': 86,\n",
       " '9_10_364_318': 87,\n",
       " '9_10_21_161': 88,\n",
       " '9_10_595_364': 89,\n",
       " '9_10_590_349': 90,\n",
       " '9_10_110_480': 91,\n",
       " '9_10_485_280': 92,\n",
       " '9_10_280_374': 93,\n",
       " '9_10_356_185': 94,\n",
       " '9_10_480_10': 95,\n",
       " '9_10_480_253': 96,\n",
       " '9_10_589_367': 97,\n",
       " '9_10_527_539': 98,\n",
       " '9_10_357_11': 99,\n",
       " '9_10_357_337': 100,\n",
       " '9_10_337_6': 101,\n",
       " '9_10_32_541': 102,\n",
       " '9_10_150_94': 103,\n",
       " '9_10_508_372': 104,\n",
       " '9_10_903_175': 105,\n",
       " '9_10_903_344': 106,\n",
       " '9_10_903_165': 107,\n",
       " '9_10_32_551': 108,\n",
       " '9_10_348_141': 109,\n",
       " '9_10_223_45': 110,\n",
       " '9_10_858_260': 111,\n",
       " '9_10_923_586': 112,\n",
       " '9_10_784_312': 113,\n",
       " '9_10_457_733': 114,\n",
       " '9_10_733_163': 115,\n",
       " '9_10_434_653': 116,\n",
       " '9_10_783_1': 117,\n",
       " '9_10_783_293': 118,\n",
       " '9_10_293_367': 119,\n",
       " '9_10_380_18': 120,\n",
       " '9_10_786_105': 121,\n",
       " '9_10_913_477': 122,\n",
       " '9_10_0_150': 123,\n",
       " '9_10_590_231': 124,\n",
       " '9_10_590_316': 125,\n",
       " '9_10_589_432': 126,\n",
       " '9_10_377_266': 127,\n",
       " '9_10_587_553': 128,\n",
       " '9_10_316_785': 129,\n",
       " '9_10_590_153': 130,\n",
       " '9_10_595_231': 131,\n",
       " '9_10_595_316': 132,\n",
       " '9_10_208_253': 133,\n",
       " '9_10_356_62': 134,\n",
       " '9_10_39_6': 135,\n",
       " '9_10_454_357': 136,\n",
       " '9_10_70_379': 137,\n",
       " '9_10_231_762': 138,\n",
       " '9_10_762_429': 139,\n",
       " '9_10_953_382': 140,\n",
       " '9_10_953_122': 141,\n",
       " '9_10_442_71': 142,\n",
       " '9_10_986_340': 143,\n",
       " '9_10_161_832': 144,\n",
       " '9_10_516_266': 145,\n",
       " '9_10_516_539': 146,\n",
       " '9_10_539_597': 147,\n",
       " '9_10_296_520': 148,\n",
       " '9_10_902_609': 149,\n",
       " '9_10_356_648': 150,\n",
       " '9_10_551_296': 151,\n",
       " '9_10_551_592': 152,\n",
       " '9_10_3_36': 153,\n",
       " '9_10_376_708': 154,\n",
       " '9_10_110_527': 155,\n",
       " '9_10_150_590': 156,\n",
       " '9_10_588_595': 157,\n",
       " '9_10_253_339': 158,\n",
       " '9_10_339_364': 159,\n",
       " '9_10_377_34': 160,\n",
       " '9_10_648_780': 161,\n",
       " '9_10_186_339': 162,\n",
       " '9_10_339_337': 163,\n",
       " '9_10_344_318': 164,\n",
       " '9_10_39_500': 165,\n",
       " '9_10_457_367': 166,\n",
       " '9_10_356_480': 167,\n",
       " '9_10_440_350': 168,\n",
       " '9_10_32_17': 169,\n",
       " '9_10_260_608': 170,\n",
       " '9_10_104_140': 171,\n",
       " '9_10_104_832': 172,\n",
       " '9_10_18_94': 173,\n",
       " '9_10_18_799': 174,\n",
       " '9_10_694_837': 175,\n",
       " '9_10_63_704': 176,\n",
       " '9_10_63_710': 177,\n",
       " '9_10_725_562': 178,\n",
       " '9_10_725_829': 179,\n",
       " '9_10_782_605': 180,\n",
       " '9_10_782_991': 181,\n",
       " '9_10_782_68': 182,\n",
       " '9_10_996_732': 183,\n",
       " '9_10_95_141': 184,\n",
       " '9_10_786_104': 185,\n",
       " '9_10_719_160': 186,\n",
       " '9_10_25_141': 187,\n",
       " '9_10_608_788': 188,\n",
       " '9_10_260_640': 189,\n",
       " '9_10_34_373': 190,\n",
       " '9_10_515_527': 191,\n",
       " '9_10_247_350': 192,\n",
       " '9_10_233_515': 193,\n",
       " '9_10_149_17': 194,\n",
       " '9_10_308_924': 195,\n",
       " '9_10_593_356': 196,\n",
       " '9_10_165_520': 197,\n",
       " '9_10_520_880': 198,\n",
       " '9_10_590_380': 199,\n",
       " '9_10_364_480': 200,\n",
       " '9_10_186_151': 201,\n",
       " '9_10_17_293': 202,\n",
       " '9_10_6_585': 203,\n",
       " '9_10_348_355': 204,\n",
       " '9_10_60_281': 205,\n",
       " '9_10_306_529': 206,\n",
       " '9_10_307_540': 207,\n",
       " '9_10_4_475': 208,\n",
       " '9_10_416_550': 209,\n",
       " '9_10_314_489': 210,\n",
       " '9_10_340_188': 211,\n",
       " '9_10_453_331': 212,\n",
       " '9_10_36_110': 213,\n",
       " '9_10_778_46': 214,\n",
       " '9_10_5_539': 215,\n",
       " '9_10_539_179': 216,\n",
       " '9_10_457_339': 217,\n",
       " '9_10_500_802': 218,\n",
       " '9_10_344_349': 219,\n",
       " '9_10_165_595': 220,\n",
       " '9_10_95_780': 221,\n",
       " '9_10_457_410': 222,\n",
       " '9_10_500_168': 223,\n",
       " '9_10_500_256': 224,\n",
       " '9_10_277_203': 225,\n",
       " '9_10_277_145': 226,\n",
       " '9_10_594_616': 227,\n",
       " '9_10_8_445': 228,\n",
       " '9_10_587_440': 229,\n",
       " '9_10_348_801': 230,\n",
       " '9_10_333_370': 231,\n",
       " '9_10_267_413': 232,\n",
       " '9_10_478_489': 233,\n",
       " '9_10_344_595': 234,\n",
       " '9_10_593_316': 235,\n",
       " '9_10_593_434': 236,\n",
       " '9_10_110_410': 237,\n",
       " '9_10_377_225': 238,\n",
       " '9_10_225_21': 239,\n",
       " '9_10_225_160': 240,\n",
       " '9_10_509_2': 241,\n",
       " '9_10_329_318': 242,\n",
       " '9_10_524_453': 243,\n",
       " '9_10_531_52': 244,\n",
       " '9_10_932_381': 245,\n",
       " '9_10_908_912': 246,\n",
       " '9_10_848_10': 247,\n",
       " '9_10_541_164': 248,\n",
       " '9_10_260_858': 249,\n",
       " '9_10_1_588': 250,\n",
       " '9_10_588_720': 251,\n",
       " '9_10_674_748': 252,\n",
       " '9_10_674_185': 253,\n",
       " '9_10_144_903': 254,\n",
       " '9_10_25_222': 255,\n",
       " '9_10_551_986': 256,\n",
       " '9_10_661_801': 257,\n",
       " '9_10_2_48': 258,\n",
       " '9_10_616_239': 259,\n",
       " '9_10_954_918': 260,\n",
       " '9_10_434_292': 261,\n",
       " '9_10_161_185': 262,\n",
       " '9_10_480_47': 263,\n",
       " '9_10_364_589': 264,\n",
       " '9_10_589_317': 265,\n",
       " '9_10_302_36': 266,\n",
       " '9_10_159_515': 267,\n",
       " '9_10_563_162': 268,\n",
       " '9_10_348_164': 269,\n",
       " '9_10_268_490': 270,\n",
       " '9_10_422_786': 271,\n",
       " '9_10_121_353': 272,\n",
       " '9_10_22_340': 273,\n",
       " '9_10_168_319': 274,\n",
       " '9_10_471_358': 275,\n",
       " '9_10_249_294': 276,\n",
       " '9_10_183_514': 277,\n",
       " '9_10_468_585': 278,\n",
       " '9_10_224_65': 279,\n",
       " '9_10_1_800': 280,\n",
       " '9_10_852_765': 281,\n",
       " '9_10_904_919': 282,\n",
       " '9_10_908_899': 283,\n",
       " '9_10_930_599': 284,\n",
       " '9_10_379_941': 285,\n",
       " '9_10_941_338': 286,\n",
       " '9_10_580_513': 287,\n",
       " '9_10_580_565': 288,\n",
       " '9_10_580_934': 289,\n",
       " '9_10_0_17': 290,\n",
       " '9_10_36_733': 291,\n",
       " '9_10_733_608': 292,\n",
       " '9_10_608_376': 293,\n",
       " '9_10_805_79': 294,\n",
       " '9_10_96_813': 295,\n",
       " '9_10_232_527': 296,\n",
       " '9_10_232_318': 297,\n",
       " '9_10_446_904': 298,\n",
       " '9_10_446_161': 299,\n",
       " '9_10_161_213': 300,\n",
       " '9_10_920_230': 301,\n",
       " '9_10_477_471': 302,\n",
       " '9_10_915_916': 303,\n",
       " '9_10_155_261': 304,\n",
       " '9_10_551_339': 305,\n",
       " '9_10_34_4': 306,\n",
       " '9_10_34_556': 307,\n",
       " '9_10_385_168': 308,\n",
       " '9_10_385_186': 309,\n",
       " '9_10_592_150': 310,\n",
       " '9_10_1_47': 311,\n",
       " '9_10_111_593': 312,\n",
       " '9_10_95_17': 313,\n",
       " '9_10_265_2': 314,\n",
       " '9_10_661_742': 315,\n",
       " '9_10_590_150': 316,\n",
       " '9_10_588_292': 317,\n",
       " '9_10_329_356': 318,\n",
       " '9_10_364_225': 319,\n",
       " '9_10_589_1': 320,\n",
       " '9_10_235_62': 321,\n",
       " '9_10_527_440': 322,\n",
       " '9_10_223_497': 323,\n",
       " '9_10_539_367': 324,\n",
       " '9_10_593_36': 325,\n",
       " '9_10_597_237': 326,\n",
       " '9_10_110_497': 327,\n",
       " '9_10_11_590': 328,\n",
       " '9_10_104_376': 329,\n",
       " '9_10_104_762': 330,\n",
       " '9_10_805_743': 331,\n",
       " '9_10_9_858': 332,\n",
       " '9_10_9_65': 333,\n",
       " '9_10_65_12': 334,\n",
       " '9_10_748_849': 335,\n",
       " '9_10_799_810': 336,\n",
       " '9_10_799_836': 337,\n",
       " '9_10_704_881': 338,\n",
       " '9_10_919_593': 339,\n",
       " '9_10_909_52': 340,\n",
       " '9_10_52_357': 341,\n",
       " '9_10_21_708': 342,\n",
       " '9_10_899_914': 343,\n",
       " '9_10_50_6': 344,\n",
       " '9_10_50_280': 345,\n",
       " '9_10_280_47': 346,\n",
       " '9_10_733_587': 347,\n",
       " '9_10_521_368': 348,\n",
       " '9_10_224_237': 349,\n",
       " '9_10_224_289': 350,\n",
       " '9_10_648_466': 351,\n",
       " '9_10_965_161': 352,\n",
       " '9_10_923_306': 353,\n",
       " '9_10_337_235': 354,\n",
       " '9_10_94_412': 355,\n",
       " '9_10_148_125': 356,\n",
       " '9_10_231_784': 357,\n",
       " '9_10_745_908': 358,\n",
       " '9_10_141_780': 359,\n",
       " '9_10_539_300': 360,\n",
       " '9_10_593_590': 361,\n",
       " '9_10_590_480': 362,\n",
       " '9_10_480_292': 363,\n",
       " '9_10_350_237': 364,\n",
       " '9_10_153_543': 365,\n",
       " '9_10_370_410': 366,\n",
       " '9_10_379_899': 367,\n",
       " '9_10_292_10': 368,\n",
       " '9_10_10_454': 369,\n",
       " '9_10_364_377': 370,\n",
       " '9_10_367_648': 371,\n",
       " '9_10_541_750': 372,\n",
       " '9_10_342_2': 373,\n",
       " '9_10_377_150': 374,\n",
       " '9_10_780_799': 375,\n",
       " '9_10_69_592': 376,\n",
       " '9_10_296_293': 377,\n",
       " '9_10_0_260': 378,\n",
       " '9_10_379_185': 379,\n",
       " '9_10_150_231': 380,\n",
       " '9_10_318_434': 381,\n",
       " '9_10_318_300': 382,\n",
       " '9_10_300_225': 383,\n",
       " '9_10_653_204': 384,\n",
       " '9_10_50_527': 385,\n",
       " '9_10_527_292': 386,\n",
       " '9_10_0_32': 387,\n",
       " '9_10_0_95': 388,\n",
       " '9_10_141_62': 389,\n",
       " '9_10_786_296': 390,\n",
       " '9_10_421_897': 391,\n",
       " '9_10_107_44': 392,\n",
       " '9_10_780_368': 393,\n",
       " '9_10_145_329': 394,\n",
       " '9_10_494_292': 395,\n",
       " '9_10_70_170': 396,\n",
       " '9_10_485_438': 397,\n",
       " '9_10_736_648': 398,\n",
       " '9_10_32_3': 399,\n",
       " '9_10_36_608': 400,\n",
       " '9_10_480_337': 401,\n",
       " '9_10_832_520': 402,\n",
       " '9_10_185_231': 403,\n",
       " '9_10_923_12': 404,\n",
       " '9_10_11_916': 405,\n",
       " '9_10_609_282': 406,\n",
       " '9_10_267_801': 407,\n",
       " '9_10_48_318': 408,\n",
       " '9_10_27_260': 409,\n",
       " '9_10_588_158': 410,\n",
       " '9_10_0_736': 411,\n",
       " '9_10_708_778': 412,\n",
       " '9_10_766_62': 413,\n",
       " '9_10_198_150': 414,\n",
       " '9_10_653_520': 415,\n",
       " '9_10_541_661': 416,\n",
       " '9_10_435_261': 417,\n",
       " '9_10_333_163': 418,\n",
       " '9_10_163_168': 419,\n",
       " '9_10_95_161': 420,\n",
       " '9_10_161_25': 421,\n",
       " '9_10_18_455': 422,\n",
       " '9_10_597_300': 423,\n",
       " '9_10_500_367': 424,\n",
       " '9_10_6_36': 425,\n",
       " '9_10_6_708': 426,\n",
       " '9_10_708_762': 427,\n",
       " '9_10_858_640': 428,\n",
       " '9_10_296_47': 429,\n",
       " '9_10_508_344': 430,\n",
       " '9_10_266_186': 431,\n",
       " '9_10_372_539': 432,\n",
       " '9_10_912_78': 433,\n",
       " '9_10_178_529': 434,\n",
       " '9_10_529_903': 435,\n",
       " '9_10_969_926': 436,\n",
       " '9_10_588_342': 437,\n",
       " '9_10_342_708': 438,\n",
       " '9_10_349_153': 439,\n",
       " '9_10_349_588': 440,\n",
       " '9_10_349_595': 441,\n",
       " '9_10_457_110': 442,\n",
       " '9_10_34_597': 443,\n",
       " '9_10_610_733': 444,\n",
       " '9_10_150_349': 445,\n",
       " '9_10_185_339': 446,\n",
       " '9_10_34_364': 447,\n",
       " '9_10_236_337': 448,\n",
       " '9_10_34_296': 449,\n",
       " '9_10_296_434': 450,\n",
       " '9_10_319_337': 451,\n",
       " '9_10_589_527': 452,\n",
       " '9_10_866_296': 453,\n",
       " '9_10_296_36': 454,\n",
       " '9_10_288_380': 455,\n",
       " '9_10_24_233': 456,\n",
       " '9_10_370_377': 457,\n",
       " '9_10_587_62': 458,\n",
       " '9_10_62_707': 459,\n",
       " '9_10_208_914': 460,\n",
       " '9_10_592_996': 461,\n",
       " '9_10_17_110': 462,\n",
       " '9_10_539_260': 463,\n",
       " '9_10_151_987': 464,\n",
       " '9_10_377_260': 465,\n",
       " '9_10_474_380': 466,\n",
       " '9_10_380_592': 467,\n",
       " '9_10_733_349': 468,\n",
       " '9_10_349_70': 469,\n",
       " '9_10_544_694': 470,\n",
       " '9_10_544_485': 471,\n",
       " '9_10_544_387': 472,\n",
       " '9_10_1_898': 473,\n",
       " '9_10_799_366': 474,\n",
       " '9_10_366_891': 475,\n",
       " '9_10_366_12': 476,\n",
       " '9_10_308_314': 477,\n",
       " '9_10_543_207': 478,\n",
       " '9_10_940_349': 479,\n",
       " '9_10_940_380': 480,\n",
       " '9_10_588_41': 481,\n",
       " '9_10_914_592': 482,\n",
       " '9_10_318_593': 483,\n",
       " '9_10_185_10': 484,\n",
       " '9_10_342_785': 485,\n",
       " '9_10_88_21': 486,\n",
       " '9_10_480_457': 487,\n",
       " '9_10_349_163': 488,\n",
       " '9_10_368_786': 489,\n",
       " '9_10_350_595': 490,\n",
       " '9_10_225_805': 491,\n",
       " '9_10_852_7': 492,\n",
       " '9_10_383_410': 493,\n",
       " '9_10_370_185': 494,\n",
       " '9_10_157_762': 495,\n",
       " '9_10_904_903': 496,\n",
       " '9_10_608_912': 497,\n",
       " '9_10_608_908': 498,\n",
       " '9_10_648_141': 499,\n",
       " '9_10_648_95': 500,\n",
       " '9_10_858_800': 501,\n",
       " '9_10_926_923': 502,\n",
       " '9_10_480_780': 503,\n",
       " '9_10_95_541': 504,\n",
       " '9_10_32_750': 505,\n",
       " '9_10_527_920': 506,\n",
       " '9_10_969_946': 507,\n",
       " '9_10_163_370': 508,\n",
       " '9_10_141_185': 509,\n",
       " '9_10_527_93': 510,\n",
       " '9_10_307_342': 511,\n",
       " '9_10_14_247': 512,\n",
       " '9_10_247_178': 513,\n",
       " '9_10_62_2': 514,\n",
       " '9_10_590_356': 515,\n",
       " '9_10_969_380': 516,\n",
       " '9_10_903_596': 517,\n",
       " '9_10_593_150': 518,\n",
       " '9_10_648_316': 519,\n",
       " '9_10_457_858': 520,\n",
       " '9_10_593_678': 521,\n",
       " '9_10_319_531': 522,\n",
       " '9_10_141_32': 523,\n",
       " '9_10_372_34': 524,\n",
       " '9_10_450_531': 525,\n",
       " '9_10_318_223': 526,\n",
       " '9_10_356_778': 527,\n",
       " '9_10_50_924': 528,\n",
       " '9_10_253_17': 529,\n",
       " '9_10_17_830': 530,\n",
       " '9_10_1_733': 531,\n",
       " '9_10_329_593': 532,\n",
       " '9_10_356_318': 533,\n",
       " '9_10_300_367': 534,\n",
       " '9_10_292_161': 535,\n",
       " '9_10_593_111': 536,\n",
       " '9_10_750_260': 537,\n",
       " '9_10_608_162': 538,\n",
       " '9_10_541_527': 539,\n",
       " '9_10_541_356': 540,\n",
       " '9_10_431_762': 541,\n",
       " '9_10_172_342': 542,\n",
       " '9_10_527_50': 543,\n",
       " '9_10_457_593': 544,\n",
       " '9_10_587_357': 545,\n",
       " '9_10_253_21': 546,\n",
       " '9_10_21_208': 547,\n",
       " '9_10_16_413': 548,\n",
       " '9_10_327_161': 549,\n",
       " '9_10_161_356': 550,\n",
       " '9_10_509_349': 551,\n",
       " '9_10_185_376': 552,\n",
       " '9_10_0_39': 553,\n",
       " '9_10_0_851': 554,\n",
       " '9_10_288_31': 555,\n",
       " '9_10_780_25': 556,\n",
       " '9_10_135_9': 557,\n",
       " '9_10_296_593': 558,\n",
       " '9_10_318_589': 559,\n",
       " '9_10_515_373': 560,\n",
       " '9_10_373_319': 561,\n",
       " '9_10_249_58': 562,\n",
       " '9_10_249_474': 563,\n",
       " '9_10_62_223': 564,\n",
       " '9_10_45_647': 565,\n",
       " '9_10_357_587': 566,\n",
       " '9_10_230_515': 567,\n",
       " '9_10_144_802': 568,\n",
       " '9_10_236_532': 569,\n",
       " '9_10_838_342': 570,\n",
       " '9_10_527_265': 571,\n",
       " '9_10_539_500': 572,\n",
       " '9_10_500_953': 573,\n",
       " '9_10_357_549': 574,\n",
       " '9_10_339_344': 575,\n",
       " '9_10_805_728': 576,\n",
       " '9_10_296_231': 577,\n",
       " '9_10_231_32': 578,\n",
       " '9_10_349_329': 579,\n",
       " '9_10_474_95': 580,\n",
       " '9_10_141_196': 581,\n",
       " '9_10_780_204': 582,\n",
       " '9_10_368_261': 583,\n",
       " '9_10_370_594': 584,\n",
       " '9_10_163_227': 585,\n",
       " '9_10_163_455': 586,\n",
       " '9_10_163_543': 587,\n",
       " '9_10_328_542': 588,\n",
       " '9_10_426_366': 589,\n",
       " '9_10_140_544': 590,\n",
       " '9_10_709_433': 591,\n",
       " '9_10_709_511': 592,\n",
       " '9_10_318_185': 593,\n",
       " '9_10_300_587': 594,\n",
       " '9_10_440_909': 595,\n",
       " '9_10_908_903': 596,\n",
       " '9_10_954_595': 597,\n",
       " '9_10_162_602': 598,\n",
       " '9_10_924_904': 599,\n",
       " '9_10_364_900': 600,\n",
       " '9_10_32_748': 601,\n",
       " '9_10_562_337': 602,\n",
       " '9_10_104_899': 603,\n",
       " '9_10_899_599': 604,\n",
       " '9_10_318_647': 605,\n",
       " '9_10_377_292': 606,\n",
       " '9_10_480_3': 607,\n",
       " '9_10_920_296': 608,\n",
       " '9_10_858_904': 609,\n",
       " '9_10_904_912': 610,\n",
       " '9_10_902_596': 611,\n",
       " '9_10_165_728': 612,\n",
       " '9_10_955_736': 613,\n",
       " '9_10_21_377': 614,\n",
       " '9_10_648_2': 615,\n",
       " '9_10_923_527': 616,\n",
       " '9_10_541_924': 617,\n",
       " '9_10_908_6': 618,\n",
       " '9_10_904_928': 619,\n",
       " '9_10_229_377': 620,\n",
       " '9_10_288_338': 621,\n",
       " '9_10_415_436': 622,\n",
       " '9_10_436_662': 623,\n",
       " '9_10_662_836': 624,\n",
       " '9_10_540_519': 625,\n",
       " '9_10_800_162': 626,\n",
       " '9_10_531_34': 627,\n",
       " '9_10_18_600': 628,\n",
       " '9_10_117_41': 629,\n",
       " '9_10_806_348': 630,\n",
       " '9_10_999_280': 631,\n",
       " '9_10_164_147': 632,\n",
       " '9_10_346_349': 633,\n",
       " '9_10_551_76': 634,\n",
       " '9_10_76_882': 635,\n",
       " '9_10_229_303': 636,\n",
       " '9_10_225_539': 637,\n",
       " '9_10_539_357': 638,\n",
       " '9_10_357_236': 639,\n",
       " '9_10_357_2': 640,\n",
       " '9_10_5_145': 641,\n",
       " '9_10_6_610': 642,\n",
       " '9_10_766_639': 643,\n",
       " '9_10_39_224': 644,\n",
       " '9_10_249_377': 645,\n",
       " '9_10_539_708': 646,\n",
       " '9_10_708_440': 647,\n",
       " '9_10_708_339': 648,\n",
       " '9_10_902_500': 649,\n",
       " '9_10_1_364': 650,\n",
       " '9_10_586_455': 651,\n",
       " '9_10_785_969': 652,\n",
       " '9_10_920_928': 653,\n",
       " '9_10_39_357': 654,\n",
       " '9_10_440_356': 655,\n",
       " '9_10_910_951': 656,\n",
       " '9_10_1_595': 657,\n",
       " '9_10_362_783': 658,\n",
       " '9_10_597_466': 659,\n",
       " '9_10_597_234': 660,\n",
       " '9_10_511_31': 661,\n",
       " '9_10_511_424': 662,\n",
       " '9_10_511_733': 663,\n",
       " '9_10_16_318': 664,\n",
       " '9_10_318_231': 665,\n",
       " '9_10_344_104': 666,\n",
       " '9_10_104_785': 667,\n",
       " '9_10_500_852': 668,\n",
       " '9_10_141_784': 669,\n",
       " '9_10_180_585': 670,\n",
       " '9_10_457_377': 671,\n",
       " '9_10_318_475': 672,\n",
       " '9_10_708_236': 673,\n",
       " '9_10_748_86': 674,\n",
       " '9_10_86_661': 675,\n",
       " '9_10_457_539': 676,\n",
       " '9_10_339_838': 677,\n",
       " '9_10_39_207': 678,\n",
       " '9_10_260_552': 679,\n",
       " '9_10_920_208': 680,\n",
       " '9_10_778_296': 681,\n",
       " '9_10_1_527': 682,\n",
       " '9_10_260_637': 683,\n",
       " '9_10_838_595': 684,\n",
       " '9_10_380_150': 685,\n",
       " '9_10_593_329': 686,\n",
       " '9_10_21_527': 687,\n",
       " '9_10_475_432': 688,\n",
       " '9_10_474_494': 689,\n",
       " '9_10_25_266': 690,\n",
       " '9_10_333_551': 691,\n",
       " '9_10_585_327': 692,\n",
       " '9_10_788_52': 693,\n",
       " '9_10_52_1': 694,\n",
       " '9_10_43_104': 695,\n",
       " '9_10_780_761': 696,\n",
       " '9_10_780_805': 697,\n",
       " '9_10_595_616': 698,\n",
       " '9_10_42_154': 699,\n",
       " '9_10_910_950': 700,\n",
       " '9_10_950_913': 701,\n",
       " '9_10_940_111': 702,\n",
       " '9_10_265_971': 703,\n",
       " '9_10_907_720': 704,\n",
       " '9_10_0_805': 705,\n",
       " '9_10_647_500': 706,\n",
       " '9_10_6_778': 707,\n",
       " '9_10_778_161': 708,\n",
       " '9_10_349_32': 709,\n",
       " '9_10_590_351': 710,\n",
       " '9_10_648_480': 711,\n",
       " '9_10_543_639': 712,\n",
       " '9_10_356_372': 713,\n",
       " '9_10_780_292': 714,\n",
       " '9_10_494_786': 715,\n",
       " '9_10_786_442': 716,\n",
       " '9_10_547_170': 717,\n",
       " '9_10_836_466': 718,\n",
       " '9_10_836_420': 719,\n",
       " '9_10_788_338': 720,\n",
       " '9_10_788_24': 721,\n",
       " '9_10_597_276': 722,\n",
       " '9_10_349_165': 723,\n",
       " '9_10_300_208': 724,\n",
       " '9_10_111_50': 725,\n",
       " '9_10_163_431': 726,\n",
       " '9_10_319_23': 727,\n",
       " '9_10_144_233': 728,\n",
       " '9_10_229_747': 729,\n",
       " '9_10_164_294': 730,\n",
       " '9_10_290_155': 731,\n",
       " '9_10_265_783': 732,\n",
       " '9_10_733_858': 733,\n",
       " '9_10_736_733': 734,\n",
       " '9_10_595_837': 735,\n",
       " '9_10_277_586': 736,\n",
       " '9_10_838_339': 737,\n",
       " '9_10_838_296': 738,\n",
       " '9_10_838_3': 739,\n",
       " '9_10_552_688': 740,\n",
       " '9_10_377_555': 741,\n",
       " '9_10_555_10': 742,\n",
       " '9_10_185_288': 743,\n",
       " '9_10_289_361': 744,\n",
       " '9_10_179_140': 745,\n",
       " '9_10_480_786': 746,\n",
       " '9_10_435_899': 747,\n",
       " '9_10_588_593': 748,\n",
       " '9_10_587_539': 749,\n",
       " '9_10_21_50': 750,\n",
       " '9_10_440_317': 751,\n",
       " '9_10_440_586': 752,\n",
       " '9_10_39_613': 753,\n",
       " '9_10_39_648': 754,\n",
       " '9_10_25_648': 755,\n",
       " '9_10_52_745': 756,\n",
       " '9_10_224_468': 757,\n",
       " '9_10_527_246': 758,\n",
       " '9_10_3_260': 759,\n",
       " '9_10_647_761': 760,\n",
       " '9_10_728_837': 761,\n",
       " '9_10_367_76': 762,\n",
       " '9_10_745_541': 763,\n",
       " '9_10_799_188': 764,\n",
       " '9_10_913_457': 765,\n",
       " '9_10_235_527': 766,\n",
       " '9_10_29_110': 767,\n",
       " '9_10_308_551': 768,\n",
       " '9_10_337_21': 769,\n",
       " '9_10_18_164': 770,\n",
       " '9_10_81_590': 771,\n",
       " '9_10_81_761': 772,\n",
       " '9_10_81_185': 773,\n",
       " '9_10_81_653': 774,\n",
       " '9_10_39_800': 775,\n",
       " '9_10_150_153': 776,\n",
       " '9_10_110_160': 777,\n",
       " '9_10_293_364': 778,\n",
       " '9_10_36_357': 779,\n",
       " '9_10_50_745': 780,\n",
       " '9_10_58_25': 781,\n",
       " '9_10_165_349': 782,\n",
       " '9_10_231_480': 783,\n",
       " '9_10_110_208': 784,\n",
       " '9_10_586_19': 785,\n",
       " '9_10_420_432': 786,\n",
       " '9_10_420_173': 787,\n",
       " '9_10_420_435': 788,\n",
       " '9_10_282_356': 789,\n",
       " '9_10_377_539': 790,\n",
       " '9_10_370_491': 791,\n",
       " '9_10_491_383': 792,\n",
       " '9_10_542_733': 793,\n",
       " '9_10_21_380': 794,\n",
       " '9_10_592_953': 795,\n",
       " '9_10_540_57': 796,\n",
       " '9_10_912_593': 797,\n",
       " '9_10_373_260': 798,\n",
       " '9_10_318_34': 799,\n",
       " '9_10_58_608': 800,\n",
       " '9_10_608_858': 801,\n",
       " '9_10_266_509': 802,\n",
       " '9_10_292_534': 803,\n",
       " '9_10_783_39': 804,\n",
       " '9_10_14_508': 805,\n",
       " '9_10_104_527': 806,\n",
       " '9_10_151_784': 807,\n",
       " '9_10_454_553': 808,\n",
       " '9_10_736_5': 809,\n",
       " '9_10_597_122': 810,\n",
       " '9_10_368_140': 811,\n",
       " '9_10_529_455': 812,\n",
       " '9_10_529_380': 813,\n",
       " '9_10_7_282': 814,\n",
       " '9_10_832_648': 815,\n",
       " '9_10_289_798': 816,\n",
       " '9_10_647_858': 817,\n",
       " '9_10_376_236': 818,\n",
       " '9_10_186_22': 819,\n",
       " '9_10_541_748': 820,\n",
       " '9_10_786_992': 821,\n",
       " '9_10_1_17': 822,\n",
       " '9_10_25_7': 823,\n",
       " '9_10_25_786': 824,\n",
       " '9_10_260_135': 825,\n",
       " '9_10_260_100': 826,\n",
       " '9_10_260_832': 827,\n",
       " '9_10_0_829': 828,\n",
       " '9_10_147_861': 829,\n",
       " '9_10_608_246': 830,\n",
       " '9_10_292_110': 831,\n",
       " '9_10_110_356': 832,\n",
       " '9_10_95_172': 833,\n",
       " '9_10_6_474': 834,\n",
       " '9_10_524_34': 835,\n",
       " '9_10_34_858': 836,\n",
       " '9_10_104_356': 837,\n",
       " '9_10_586_231': 838,\n",
       " '9_10_380_153': 839,\n",
       " '9_10_595_318': 840,\n",
       " '9_10_356_337': 841,\n",
       " '9_10_480_440': 842,\n",
       " '9_10_25_539': 843,\n",
       " '9_10_587_348': 844,\n",
       " '9_10_163_230': 845,\n",
       " '9_10_322_481': 846,\n",
       " '9_10_281_371': 847,\n",
       " '9_10_155_724': 848,\n",
       " '9_10_475_314': 849,\n",
       " '9_10_73_786': 850,\n",
       " '9_10_73_101': 851,\n",
       " '9_10_589_253': 852,\n",
       " '9_10_589_47': 853,\n",
       " '9_10_457_589': 854,\n",
       " '9_10_153_24': 855,\n",
       " '9_10_350_493': 856,\n",
       " '9_10_350_178': 857,\n",
       " '9_10_350_521': 858,\n",
       " '9_10_350_699': 859,\n",
       " '9_10_350_534': 860,\n",
       " '9_10_534_263': 861,\n",
       " '9_10_277_159': 862,\n",
       " '9_10_592_354': 863,\n",
       " '9_10_592_458': 864,\n",
       " '9_10_592_270': 865,\n",
       " '9_10_592_271': 866,\n",
       " '9_10_592_340': 867,\n",
       " '9_10_592_365': 868,\n",
       " '9_10_592_444': 869,\n",
       " '9_10_592_209': 870,\n",
       " '9_10_69_163': 871,\n",
       " '9_10_344_180': 872,\n",
       " '9_10_514_252': 873,\n",
       " '9_10_237_550': 874,\n",
       " '9_10_415_310': 875,\n",
       " '9_10_415_276': 876,\n",
       " '9_10_415_234': 877,\n",
       " '9_10_248_274': 878,\n",
       " '9_10_548_71': 879,\n",
       " '9_10_253_320': 880,\n",
       " '9_10_253_490': 881,\n",
       " '9_10_253_540': 882,\n",
       " '9_10_253_132': 883,\n",
       " '9_10_262_362': 884,\n",
       " '9_10_596_258': 885,\n",
       " '9_10_596_484': 886,\n",
       " '9_10_596_48': 887,\n",
       " '9_10_596_37': 888,\n",
       " '9_10_596_13': 889,\n",
       " '9_10_361_789': 890,\n",
       " '9_10_361_232': 891,\n",
       " '9_10_361_681': 892,\n",
       " '9_10_236_788': 893,\n",
       " '9_10_593_110': 894,\n",
       " '9_10_593_296': 895,\n",
       " '9_10_150_508': 896,\n",
       " '9_10_589_733': 897,\n",
       " '9_10_456_16': 898,\n",
       " '9_10_16_111': 899,\n",
       " '9_10_356_110': 900,\n",
       " '9_10_150_780': 901,\n",
       " '9_10_587_161': 902,\n",
       " '9_10_253_25': 903,\n",
       " '9_10_25_62': 904,\n",
       " '9_10_852_52': 905,\n",
       " '9_10_383_953': 906,\n",
       " '9_10_222_367': 907,\n",
       " '9_10_222_830': 908,\n",
       " '9_10_351_218': 909,\n",
       " '9_10_260_527': 910,\n",
       " '9_10_293_1': 911,\n",
       " '9_10_337_904': 912,\n",
       " '9_10_101_539': 913,\n",
       " '9_10_509_586': 914,\n",
       " '9_10_288_371': 915,\n",
       " '9_10_52_185': 916,\n",
       " '9_10_52_342': 917,\n",
       " '9_10_543_900': 918,\n",
       " '9_10_785_968': 919,\n",
       " '9_10_968_24': 920,\n",
       " '9_10_356_277': 921,\n",
       " '9_10_587_36': 922,\n",
       " '9_10_36_994': 923,\n",
       " '9_10_508_339': 924,\n",
       " '9_10_508_376': 925,\n",
       " '9_10_0_923': 926,\n",
       " '9_10_858_348': 927,\n",
       " '9_10_348_364': 928,\n",
       " '9_10_17_971': 929,\n",
       " '9_10_17_909': 930,\n",
       " '9_10_915_164': 931,\n",
       " '9_10_835_947': 932,\n",
       " '9_10_947_479': 933,\n",
       " '9_10_55_728': 934,\n",
       " '9_10_940_260': 935,\n",
       " '9_10_940_329': 936,\n",
       " '9_10_969_344': 937,\n",
       " '9_10_969_551': 938,\n",
       " '9_10_969_253': 939,\n",
       " '9_10_951_694': 940,\n",
       " '9_10_951_277': 941,\n",
       " '9_10_262_31': 942,\n",
       " '9_10_175_176': 943,\n",
       " '9_10_176_941': 944,\n",
       " '9_10_766_190': 945,\n",
       " '9_10_750_305': 946,\n",
       " '9_10_720_95': 947,\n",
       " '9_10_913_916': 948,\n",
       " '9_10_950_949': 949,\n",
       " '9_10_950_306': 950,\n",
       " '9_10_111_158': 951,\n",
       " '9_10_904_232': 952,\n",
       " '9_10_232_477': 953,\n",
       " '9_10_232_532': 954,\n",
       " '9_10_232_45': 955,\n",
       " '9_10_45_187': 956,\n",
       " '9_10_45_539': 957,\n",
       " '9_10_45_410': 958,\n",
       " '9_10_32_1': 959,\n",
       " '9_10_62_5': 960,\n",
       " '9_10_62_6': 961,\n",
       " '9_10_36_3': 962,\n",
       " '9_10_802_135': 963,\n",
       " '9_10_79_14': 964,\n",
       " '9_10_86_61': 965,\n",
       " '9_10_61_849': 966,\n",
       " '9_10_64_694': 967,\n",
       " '9_10_349_593': 968,\n",
       " '9_10_434_356': 969,\n",
       " '9_10_47_300': 970,\n",
       " '9_10_377_527': 971,\n",
       " '9_10_47_2': 972,\n",
       " '9_10_722_296': 973,\n",
       " '9_10_720_527': 974,\n",
       " '9_10_307_593': 975,\n",
       " '9_10_994_319': 976,\n",
       " '9_10_529_608': 977,\n",
       " '9_10_509_21': 978,\n",
       " '9_10_151_431': 979,\n",
       " '9_10_431_175': 980,\n",
       " '9_10_175_628': 981,\n",
       " '9_10_532_42': 982,\n",
       " '9_10_42_464': 983,\n",
       " '9_10_1_62': 984,\n",
       " '9_10_14_838': 985,\n",
       " '9_10_838_661': 986,\n",
       " '9_10_141_25': 987,\n",
       " '9_10_733_6': 988,\n",
       " '9_10_628_140': 989,\n",
       " '9_10_653_74': 990,\n",
       " '9_10_653_737': 991,\n",
       " '9_10_653_92': 992,\n",
       " '9_10_653_662': 993,\n",
       " '9_10_153_595': 994,\n",
       " '9_10_153_231': 995,\n",
       " '9_10_34_587': 996,\n",
       " '9_10_34_19': 997,\n",
       " '9_10_19_225': 998,\n",
       " '9_10_173_736': 999,\n",
       " '9_10_173_551': 1000,\n",
       " '9_10_158_353': 1001,\n",
       " '9_10_158_48': 1002,\n",
       " '9_10_112_413': 1003,\n",
       " '9_10_112_188': 1004,\n",
       " '9_10_588_356': 1005,\n",
       " '9_10_589_231': 1006,\n",
       " '9_10_908_151': 1007,\n",
       " '9_10_339_380': 1008,\n",
       " '9_10_594_899': 1009,\n",
       " '9_10_595_661': 1010,\n",
       " '9_10_11_39': 1011,\n",
       " '9_10_605_34': 1012,\n",
       " '9_10_104_420': 1013,\n",
       " '9_10_104_719': 1014,\n",
       " '9_10_104_216': 1015,\n",
       " '9_10_104_344': 1016,\n",
       " '9_10_372_520': 1017,\n",
       " '9_10_608_457': 1018,\n",
       " '9_10_494_798': 1019,\n",
       " ...}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_name_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43404"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_name_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ModelDataSet(Dataset):\n",
    "    # Retrieve an item in every call\n",
    "    def __init__(self, input_DF, label_DF, sparse_col, dense_col, cross_col, cross_col_table):\n",
    "        self.df = input_DF\n",
    "        \n",
    "        self.dense_df = input_DF.iloc[:, dense_col].astype(np.float32) \n",
    "        self.sparse_df = input_DF.iloc[:, sparse_col].astype('int64') \n",
    "        \n",
    "        self.cross_col = cross_col\n",
    "        self.cross_col_table = cross_col_table\n",
    "        self.cross_df = self._transform_cross_col().astype('int64')   # transform cross_col to label encoding\n",
    "        \n",
    "        self.label = label_DF.astype(np.float32) \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sparse_feature = torch.tensor(self.sparse_df.iloc[idx])\n",
    "        dense_feature = torch.tensor(self.dense_df.iloc[idx])\n",
    "        cross_feature = torch.tensor(self.cross_df.iloc[idx])\n",
    "        \n",
    "        label = torch.tensor(self.label.iloc[idx])\n",
    "        return {'Feature': (sparse_feature, dense_feature, cross_feature), 'Label': label}\n",
    "    \n",
    "    def _transform_cross_col(self):\n",
    "        # label enbedding cross_col based on cross_col_table\n",
    "        new_df = pd.DataFrame()\n",
    "        for col_1_index, col_2_index in self.cross_col:\n",
    "            new_col_val = []\n",
    "            new_col_name = str(col_1_index) + \"_\" + str(col_2_index)\n",
    "            for cell_1, cell_2 in zip(self.df.iloc[:, col_1_index], self.df.iloc[:, col_2_index]):\n",
    "                feature_name = str(col_1_index) + \"_\" + str(col_2_index) + \"_\" + str(cell_1) + \"_\" + str(cell_2)\n",
    "                encoding_label = self.cross_col_table[feature_name] if feature_name in self.cross_col_table else 0\n",
    "                new_col_val.append(encoding_label)\n",
    "            new_df[new_col_name] = new_col_val\n",
    "        return new_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_dataset = ModelDataSet(training_feature, training_label, sparse_col, dense_col, cross_col, feature_name_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ModelDataSet(test_feature, test_label, sparse_col, dense_col, cross_col, feature_name_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(self, sparse_col_size, dense_col_size, cross_feature_size):\n",
    "        super().__init__()\n",
    "        self.sparse_col_size = sparse_col_size\n",
    "        self.dense_col_size = dense_col_size\n",
    "        self.cross_feature_size = cross_feature_size\n",
    "        # For categorical features, we embed the features in dense vectors of dimension of 6 * category cardinality^1/4\n",
    "        embedding_size = list(map(lambda x: int(6 * pow(x, 0.25)), self.sparse_col_size))\n",
    "        \n",
    "        # Create embedding layer for all sparse features\n",
    "        sparse_embedding_list = []\n",
    "        for class_size, embed_size in zip(self.sparse_col_size, embedding_size):\n",
    "            sparse_embedding_list.append(nn.Embedding(class_size, embed_size, scale_grad_by_freq=True))\n",
    "        self.sparse_embedding_layer = nn.ModuleList(sparse_embedding_list)\n",
    "        \n",
    "        # Cal total embedding size \n",
    "        total_embedding_size = np.sum(embedding_size) + self.dense_col_size\n",
    "        \n",
    "        # Deep side linear layers\n",
    "        self.linear1 = nn.Linear(total_embedding_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 128)\n",
    "        \n",
    "        # final linear layer\n",
    "        self.linear3 = nn.Linear(128+self.cross_feature_size+20, 1) \n",
    "        # 20 is single_feature_size. Movie_gerne label is from 0 to 19\n",
    "        \n",
    "    def forward(self, sparse_feature, dense_feature, cross_feature):\n",
    "        if (len(sparse_feature.shape) == 1): # 1D tensor coverted to 2D tensor if batch_number == 1\n",
    "            sparse_feature = sparse_feature.view(1, -1)\n",
    "            dense_feature = dense_feature.view(1, -1)\n",
    "            cross_feature = cross_feature.view(1, -1)\n",
    "        \n",
    "        # Deep side\n",
    "        embedding_list = []\n",
    "        sparse_feature_size = sparse_feature.shape[1]\n",
    "        for i in range(sparse_feature_size):\n",
    "            sparse_feature_input = sparse_feature[:, i] # batch x 1\n",
    "            embedding_layer = self.sparse_embedding_layer[i]\n",
    "            embedding_output = embedding_layer(sparse_feature_input) # batch x 1 x embedding_size\n",
    "            embedding_list.append(embedding_output.squeeze(1)) # batch x embedding_size\n",
    "        embedding = torch.cat(embedding_list, dim=1) # batch x sum(embedding_size)\n",
    "        embedding = torch.cat([embedding, dense_feature], dim=1) # batch x (sum(embedding_size)+dense_feature_size)\n",
    "        deep_output = F.relu(self.linear1(embedding)) # batch x 128\n",
    "        deep_output = F.relu(self.linear2(deep_output)) # batch x 128\n",
    "        \n",
    "        # Wide side\n",
    "        class_number = self.cross_feature_size+20\n",
    "        one_hot_vec = F.one_hot(cross_feature, num_classes=class_number).squeeze(1) # batch x class_number\n",
    "        \n",
    "        # Concate and feed to final layer\n",
    "        final_input = torch.cat([deep_output, one_hot_vec], dim=1)\n",
    "        output = F.sigmoid(self.linear3(final_input)) # batch x 1\n",
    "        return output.view(-1) # (batch,)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideAndDeep(sparse_col_size, 7, len(feature_name_to_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Eval():\n",
    "    def __init__(self, model, loss_fn, optim, device, train_dataloader, test_dataloader):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.optim = optim\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.threashold = 0.5 # threashold for positive class\n",
    "        \n",
    "    def train(self, epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            print(\"==========================================================\")\n",
    "            print(\"start training epoch: {}\".format(epoch+1))\n",
    "            loss_list = []\n",
    "            pred_list = []\n",
    "            label_list = []\n",
    "            \n",
    "            iteration = 1\n",
    "            for train_data in self.train_dataloader:\n",
    "                sparse_feature = train_data['Feature'][0].to(self.device)\n",
    "                dense_feature = train_data['Feature'][1].to(self.device)\n",
    "                cross_feature = train_data['Feature'][2].to(self.device)\n",
    "                label = train_data['Label'].to(self.device)\n",
    "                prediction = self.model(sparse_feature, dense_feature, cross_feature)\n",
    "                \n",
    "                pred_list.extend(prediction.tolist())\n",
    "                label_list.extend(label.tolist())\n",
    "                \n",
    "                cur_loss = self.loss_fn(prediction, label)\n",
    "                loss_list.append(cur_loss.item())\n",
    "                cur_loss.backward()\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "                \n",
    "                # logging every 20 iteration\n",
    "                if iteration % 20 == 0:\n",
    "                    print(\"---------------------------------------------------------\")\n",
    "                    print(\"epoch {}/{}, cur_iteration is {}, logloss is {:.2f}\"\n",
    "                          .format(epoch+1, epochs, iteration, cur_loss.item()))\n",
    "                iteration += 1\n",
    "                \n",
    "            # validation every epoch\n",
    "            training_loss, training_accuracy, training_roc_score = self._getMetric(loss_list, pred_list, label_list)\n",
    "            print(\"==========================================================\")\n",
    "            print(\"Result of epoch {}\".format(epoch+1))\n",
    "            print(f\"training loss: {training_loss:.2f}, accuracy: {training_accuracy:.3f}, roc_score: {training_roc_score:.2f}\")\n",
    "            \n",
    "            test_loss, test_accuracy, test_roc_score = self.eval()\n",
    "            print(f\"test loss: {test_loss:.2f}, accuracy: {test_accuracy:.3f}, roc_score: {test_roc_score:.2f}\")\n",
    "            # summary.add_embedding(np.reshape(np.array(loss_list), (1, -1)), tag=\"loss_list\")\n",
    "            # summary.add_embedding(np.reshape(np.array(pred_list), (1, -1)), tag=\"pred_list\")\n",
    "            # summary.add_embedding(np.reshape(np.array(label_list), (1, -1)), tag=\"label_list\")\n",
    "            # summary.add_scalar(\"training_loss\", training_loss)\n",
    "            # summary.add_scalar(\"training_accuracy\", training_accuracy)\n",
    "            # summary.add_scalar(\"training_roc_score\", training_roc_score)\n",
    "    \n",
    "    def eval(self):\n",
    "        # return logloss, accuracy, roc_score\n",
    "        self.model.eval()\n",
    "        loss_list = []\n",
    "        pred_list = []\n",
    "        label_list = []\n",
    "        with torch.no_grad():\n",
    "            for test_data in self.test_dataloader:\n",
    "                sparse_feature = test_data['Feature'][0].to(self.device)\n",
    "                dense_feature = test_data['Feature'][1].to(self.device)\n",
    "                cross_feature = test_data['Feature'][2].to(self.device)\n",
    "                label = test_data['Label'].to(self.device)\n",
    "                prediction = self.model(sparse_feature, dense_feature, cross_feature)\n",
    "                cur_loss = self.loss_fn(prediction, label)\n",
    "                \n",
    "                loss_list.append(cur_loss.item())\n",
    "                pred_list.extend(prediction.tolist())\n",
    "                label_list.extend(label.tolist())\n",
    "        return self._getMetric(loss_list, pred_list, label_list)\n",
    "                \n",
    "    def _getMetric(self, loss_list, pred_list, label_list):\n",
    "        # return logloss, accuracy, roc_score        \n",
    "        # average logloss\n",
    "        avg_loss = np.mean(loss_list)\n",
    "        # roc_score\n",
    "        roc_score = roc_auc_score(label_list, pred_list)\n",
    "        # average accuracy\n",
    "        pred_class_list = list(map(lambda x: 1 if x >= self.threashold else 0, pred_list))\n",
    "        correct_count = 0\n",
    "        for p, l in zip(pred_class_list, label_list):\n",
    "            if p == l:\n",
    "                correct_count += 1\n",
    "        avg_accuracy = correct_count / len(label_list)\n",
    "        \n",
    "        return avg_loss, avg_accuracy, roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_eval = Train_Eval(model, loss_fn, optimizer, dev, training_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "start training epoch: 1\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 20, logloss is 0.70\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 40, logloss is 0.69\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 60, logloss is 0.68\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 80, logloss is 0.65\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 100, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 120, logloss is 0.64\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 140, logloss is 0.65\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 160, logloss is 0.65\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 180, logloss is 0.66\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 200, logloss is 0.64\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 220, logloss is 0.61\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 240, logloss is 0.65\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 260, logloss is 0.66\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 280, logloss is 0.66\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 300, logloss is 0.65\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 320, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 340, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 360, logloss is 0.66\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 380, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 400, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 420, logloss is 0.67\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 440, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 460, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 480, logloss is 0.61\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 500, logloss is 0.67\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 520, logloss is 0.69\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 540, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 560, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 580, logloss is 0.64\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 600, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 620, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 640, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 660, logloss is 0.53\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 680, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 700, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 720, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 740, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 760, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 780, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 800, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 820, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 1/5, cur_iteration is 840, logloss is 0.62\n",
      "==========================================================\n",
      "Result of epoch 1\n",
      "training loss: 0.63, accuracy: 0.639, roc_score: 0.69\n",
      "test loss: 0.57, accuracy: 0.713, roc_score: 0.76\n",
      "==========================================================\n",
      "start training epoch: 2\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 20, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 40, logloss is 0.64\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 60, logloss is 0.61\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 80, logloss is 0.61\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 100, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 120, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 140, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 160, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 180, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 200, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 220, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 240, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 260, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 280, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 300, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 320, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 340, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 360, logloss is 0.61\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 380, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 400, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 420, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 440, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 460, logloss is 0.53\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 480, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 500, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 520, logloss is 0.61\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 540, logloss is 0.61\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 560, logloss is 0.64\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 580, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 600, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 620, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 640, logloss is 0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 660, logloss is 0.67\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 680, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 700, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 720, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 740, logloss is 0.50\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 760, logloss is 0.52\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 780, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 800, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 820, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 2/5, cur_iteration is 840, logloss is 0.59\n",
      "==========================================================\n",
      "Result of epoch 2\n",
      "training loss: 0.58, accuracy: 0.690, roc_score: 0.76\n",
      "test loss: 0.53, accuracy: 0.745, roc_score: 0.79\n",
      "==========================================================\n",
      "start training epoch: 3\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 20, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 40, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 60, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 80, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 100, logloss is 0.52\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 120, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 140, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 160, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 180, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 200, logloss is 0.46\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 220, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 240, logloss is 0.53\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 260, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 280, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 300, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 320, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 340, logloss is 0.53\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 360, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 380, logloss is 0.52\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 400, logloss is 0.47\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 420, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 440, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 460, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 480, logloss is 0.49\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 500, logloss is 0.61\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 520, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 540, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 560, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 580, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 600, logloss is 0.50\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 620, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 640, logloss is 0.61\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 660, logloss is 0.50\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 680, logloss is 0.52\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 700, logloss is 0.53\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 720, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 740, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 760, logloss is 0.49\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 780, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 800, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 820, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 3/5, cur_iteration is 840, logloss is 0.52\n",
      "==========================================================\n",
      "Result of epoch 3\n",
      "training loss: 0.57, accuracy: 0.701, roc_score: 0.77\n",
      "test loss: 0.54, accuracy: 0.736, roc_score: 0.79\n",
      "==========================================================\n",
      "start training epoch: 4\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 20, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 40, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 60, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 80, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 100, logloss is 0.53\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 120, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 140, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 160, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 180, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 200, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 220, logloss is 0.61\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 240, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 260, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 280, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 300, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 320, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 340, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 360, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 380, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 400, logloss is 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 420, logloss is 0.52\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 440, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 460, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 480, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 500, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 520, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 540, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 560, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 580, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 600, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 620, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 640, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 660, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 680, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 700, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 720, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 740, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 760, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 780, logloss is 0.53\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 800, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 820, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 4/5, cur_iteration is 840, logloss is 0.52\n",
      "==========================================================\n",
      "Result of epoch 4\n",
      "training loss: 0.57, accuracy: 0.706, roc_score: 0.77\n",
      "test loss: 0.52, accuracy: 0.743, roc_score: 0.79\n",
      "==========================================================\n",
      "start training epoch: 5\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 20, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 40, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 60, logloss is 0.55\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 80, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 100, logloss is 0.68\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 120, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 140, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 160, logloss is 0.52\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 180, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 200, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 220, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 240, logloss is 0.47\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 260, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 280, logloss is 0.60\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 300, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 320, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 340, logloss is 0.53\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 360, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 380, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 400, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 420, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 440, logloss is 0.54\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 460, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 480, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 500, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 520, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 540, logloss is 0.62\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 560, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 580, logloss is 0.56\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 600, logloss is 0.65\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 620, logloss is 0.49\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 640, logloss is 0.50\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 660, logloss is 0.64\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 680, logloss is 0.59\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 700, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 720, logloss is 0.57\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 740, logloss is 0.63\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 760, logloss is 0.47\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 780, logloss is 0.51\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 800, logloss is 0.58\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 820, logloss is 0.53\n",
      "---------------------------------------------------------\n",
      "epoch 5/5, cur_iteration is 840, logloss is 0.55\n",
      "==========================================================\n",
      "Result of epoch 5\n",
      "training loss: 0.56, accuracy: 0.709, roc_score: 0.78\n",
      "test loss: 0.52, accuracy: 0.746, roc_score: 0.80\n"
     ]
    }
   ],
   "source": [
    "train_eval.train(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of WideAndDeep(\n",
       "  (sparse_embedding_layer): ModuleList(\n",
       "    (0): Embedding(30001, 78, scale_grad_by_freq=True)\n",
       "    (1): Embedding(20, 12, scale_grad_by_freq=True)\n",
       "    (2): Embedding(20, 12, scale_grad_by_freq=True)\n",
       "    (3): Embedding(20, 12, scale_grad_by_freq=True)\n",
       "    (4): Embedding(20, 12, scale_grad_by_freq=True)\n",
       "    (5): Embedding(20, 12, scale_grad_by_freq=True)\n",
       "    (6): Embedding(1001, 33, scale_grad_by_freq=True)\n",
       "    (7): Embedding(20, 12, scale_grad_by_freq=True)\n",
       "    (8): Embedding(20, 12, scale_grad_by_freq=True)\n",
       "    (9): Embedding(20, 12, scale_grad_by_freq=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=214, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (linear3): Linear(in_features=43552, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
